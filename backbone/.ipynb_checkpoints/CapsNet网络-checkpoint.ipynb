{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 胶囊网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T12:36:01.896051Z",
     "start_time": "2020-05-18T12:36:01.892062Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./capsnet.jpg)\n",
    "\n",
    "**结构解读**\n",
    "\n",
    "- 普通卷积层 $Conv1$：基本的卷积层，感受野较大，达到了 $9*9$\n",
    "\n",
    "- 预胶囊层 $PrimaryCaps$：为胶囊层准备，运算为卷积运算，最终输出为[batch,caps_num,caps_length]的三维数据：batch为批大小\n",
    "\n",
    "- 胶囊层 $DigitCaps$：胶囊层，目的是代替最后一层全连接层，输出为 $10$ 个胶囊\n",
    "\n",
    "    - $caps\\_num$ 为胶囊的数量 图中为 $10$\n",
    "\n",
    "    - $caps\\_length$ 为每个胶囊的长度（每个胶囊为一个向量，该向量包括 $caps\\_length$ 个分量） 图中为 $16$\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 胶囊网络组件的代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T12:28:20.730418Z",
     "start_time": "2020-05-18T12:28:20.725449Z"
    }
   },
   "source": [
    "**胶囊网络激活函数**\n",
    "\n",
    "![](./squash.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T12:28:44.199268Z",
     "start_time": "2020-05-18T12:28:44.195280Z"
    }
   },
   "outputs": [],
   "source": [
    "def squash(inputs, axis=-1):\n",
    "    norm = torch.norm(inputs, p=2, dim=axis, keepdim=True)\n",
    "    scale = norm**2 / (1 + norm**2) / (norm + 1e-8)\n",
    "    return scale * inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预胶囊层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T12:36:07.830047Z",
     "start_time": "2020-05-18T12:36:07.824058Z"
    }
   },
   "outputs": [],
   "source": [
    "class PrimaryCapsule(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply Conv2D with `out_channels` and then reshape to get capsules\n",
    "    :param in_channels: input channels\n",
    "    :param out_channels: output channels\n",
    "    :param dim_caps: dimension of capsule\n",
    "    :param kernel_size: kernel size\n",
    "    :return: output tensor, size=[batch, num_caps, dim_caps]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dim_caps, kernel_size, stride=1, padding=0):\n",
    "        super(PrimaryCapsule, self).__init__()\n",
    "        self.dim_caps = dim_caps  # 每个胶囊的长度\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "    def forward(self, x):\n",
    "        outputs = self.conv2d(x)\n",
    "        outputs = outputs.view(x.size(0), -1, self.dim_caps) # [batch,caps_num,caps_length]\n",
    "        return squash(outputs) # 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 胶囊层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**动态路由算法**\n",
    "\n",
    "![](./动态路由.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T13:00:50.351735Z",
     "start_time": "2020-05-18T13:00:50.342754Z"
    }
   },
   "outputs": [],
   "source": [
    "class DenseCapsule(nn.Module):\n",
    "    \"\"\"\n",
    "    in_num_caps：输入胶囊的数量\n",
    "    in_dim_caps：输入胶囊的长度（维数）\n",
    "    out_num_caps：输出胶囊的数量\n",
    "    out_dim_caps：输出胶囊的长度（维数）\n",
    "    routings：动态路由迭代的次数\n",
    "    \"\"\"\n",
    "    def __init__(self, in_num_caps, in_dim_caps, out_num_caps, out_dim_caps, routings=3):\n",
    "        super(DenseCapsule, self).__init__()\n",
    "        self.in_num_caps = in_num_caps\n",
    "        self.in_dim_caps = in_dim_caps\n",
    "        self.out_num_caps = out_num_caps\n",
    "        self.out_dim_caps = out_dim_caps\n",
    "        self.routings = routings\n",
    "        # 定义了权值weight，尺寸为[out_num_caps, in_num_caps, out_dim_caps, in_dim_caps]，即每个输出和每个输出胶囊都有连接\n",
    "        self.weight = nn.Parameter(0.01 * torch.randn(out_num_caps, in_num_caps, out_dim_caps, in_dim_caps))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        第一部分： 输入映射部分\n",
    "        - x[:, None, :, :, None]将数据维度从[batch, in_num_caps, in_dim_caps]扩展到[batch, 1,in_num_caps, in_dim_caps,1]\n",
    "        \n",
    "        - torch.matmul()将weight和扩展后的输入相乘，weight的尺寸是[out_num_caps, in_num_caps, out_dim_caps, in_dim_caps]，\n",
    "          相乘后结果尺寸为[batch, out_num_caps, in_num_caps,out_dim_caps, 1]\n",
    "        \n",
    "        - torch.squeeze去除多余维度 [batch, out_num_caps, in_num_caps,out_dim_caps]\n",
    "        '''\n",
    "        x_hat = torch.squeeze(torch.matmul(self.weight, x[:, None, :, :, None]), dim=-1)\n",
    "        x_hat_detached = x_hat.detach() # 截断梯度，禁止反向传播\n",
    "        \n",
    "        '''\n",
    "        第二部分： 动态路由算法\n",
    "        \n",
    "        - 第一部分是softmax函数，使用c = F.softmax(b, dim=1)实现，该步骤不改变b的尺寸\n",
    "        \n",
    "        - 第二部分是计算路由结果：outputs = squash(torch.sum(c[:, :, :, None] * x_hat, dim=-2, keepdim=True))\n",
    "          c[:, :, :, None]扩展c的维度，以便按位置相乘时广播维度\n",
    "         torch.sum(c[:, :, :, None] * x_hat, dim=-2, keepdim=True)计算出每个胶囊与对应权值的积，即算法中的sj，\n",
    "         同时在倒数第二维上求和，则该步输出的结果尺寸为[batch, out_num_caps, 1,out_dim_caps] 通过激活函数squash()\n",
    "        '''\n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        # b.size = [batch, out_num_caps, in_num_caps]\n",
    "        b = torch.zeros(x.size(0), self.out_num_caps, self.in_num_caps)\n",
    "\n",
    "        assert self.routings > 0, 'The \\'routings\\' should be > 0.'\n",
    "        for i in range(self.routings):\n",
    "            # c.size = [batch, out_num_caps, in_num_caps]\n",
    "            c = F.softmax(b, dim=1)\n",
    "\n",
    "            # At last iteration, use `x_hat` to compute `outputs` in order to backpropagate gradient\n",
    "            if i == self.routings - 1:\n",
    "                # c.size expanded to [batch, out_num_caps, in_num_caps, 1           ]\n",
    "                # x_hat.size     =   [batch, out_num_caps, in_num_caps, out_dim_caps]\n",
    "                # => outputs.size=   [batch, out_num_caps, 1,           out_dim_caps]\n",
    "                outputs = squash(torch.sum(c[:, :, :, None] * x_hat, dim=-2, keepdim=True))\n",
    "                # outputs = squash(torch.matmul(c[:, :, None, :], x_hat))  # alternative way\n",
    "            else:  # Otherwise, use `x_hat_detached` to update `b`. No gradients flow on this path.\n",
    "                outputs = squash(torch.sum(c[:, :, :, None] * x_hat_detached, dim=-2, keepdim=True))\n",
    "                # outputs = squash(torch.matmul(c[:, :, None, :], x_hat_detached))  # alternative way\n",
    "\n",
    "                # outputs.size       =[batch, out_num_caps, 1,           out_dim_caps]\n",
    "                # x_hat_detached.size=[batch, out_num_caps, in_num_caps, out_dim_caps]\n",
    "                # => b.size          =[batch, out_num_caps, in_num_caps]\n",
    "                b = b + torch.sum(outputs * x_hat_detached, dim=-1)\n",
    "\n",
    "        return torch.squeeze(outputs, dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T13:03:41.068798Z",
     "start_time": "2020-05-18T13:03:41.063812Z"
    }
   },
   "outputs": [],
   "source": [
    "in_num_caps = 32\n",
    "in_dim_caps = 6\n",
    "out_num_caps = 10\n",
    "out_dim_caps = 16\n",
    "weight = torch.randn(out_num_caps, in_num_caps, out_dim_caps, in_dim_caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T13:04:07.909462Z",
     "start_time": "2020-05-18T13:04:07.898491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 16, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T13:09:17.501418Z",
     "start_time": "2020-05-18T13:09:17.495405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32, 6, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1,in_num_caps, in_dim_caps)\n",
    "x = x[:, None, :, :, None]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T13:09:20.076928Z",
     "start_time": "2020-05-18T13:09:19.891908Z"
    }
   },
   "outputs": [],
   "source": [
    "res = torch.matmul(weight, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T13:09:25.606167Z",
     "start_time": "2020-05-18T13:09:25.600150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 32, 16, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 胶囊网络整体结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T13:23:29.788652Z",
     "start_time": "2020-05-18T13:23:29.779677Z"
    }
   },
   "outputs": [],
   "source": [
    "class CapsuleNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A Capsule Network on MNIST.\n",
    "    :param input_size: data size = [channels, width, height]\n",
    "    :param classes: number of classes\n",
    "    :param routings: number of routing iterations\n",
    "    Shape:\n",
    "        - Input: (batch, channels, width, height), optional (batch, classes) .\n",
    "        - Output:((batch, classes), (batch, channels, width, height))\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, classes, routings):\n",
    "        super(CapsuleNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.classes = classes\n",
    "        self.routings = routings\n",
    "\n",
    "        # Layer 1: Just a conventional Conv2D layer\n",
    "        self.conv1 = nn.Conv2d(input_size[0],\n",
    "                               256,\n",
    "                               kernel_size=9,\n",
    "                               stride=1,\n",
    "                               padding=0)\n",
    "\n",
    "        # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_caps, dim_caps]\n",
    "        self.primarycaps = PrimaryCapsule(256,\n",
    "                                          256,\n",
    "                                          8,\n",
    "                                          kernel_size=9,\n",
    "                                          stride=2,\n",
    "                                          padding=0)\n",
    "\n",
    "        # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "        self.digitcaps = DenseCapsule(in_num_caps=32 * 6 * 6,\n",
    "                                      in_dim_caps=8,\n",
    "                                      out_num_caps=classes,\n",
    "                                      out_dim_caps=16,\n",
    "                                      routings=routings)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.primarycaps(x)\n",
    "        x = self.digitcaps(x)\n",
    "        length = x.norm(dim=-1)\n",
    "        return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
